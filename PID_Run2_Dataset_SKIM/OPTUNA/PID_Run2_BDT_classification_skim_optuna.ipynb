{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Photon ID Run 2 BDT classification - Hyperparameter optimation\n",
    "\n",
    "https://github.com/optuna/optuna-examples/blob/main/lightgbm/lightgbm_simple.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "import seaborn as sns\n",
    "\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import RocCurveDisplay, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"/home/chardong/y_identification/Venv/save_pkl/\"\n",
    "savedir = \"/home/chardong/y_identification/Venv/save_plots/Py8_yj_jj_train_skim30/\"\n",
    "# Chemin pour enregistrer les fichiers pickle\n",
    "save_path = '/home/chardong/y_identification/Venv/save_pkl/Fudge_Factor/'\n",
    "#datadir = \"/eos/user/m/mdelmast/Data/EGamma/PhotonID/Run2/\"\n",
    "savedirmodel = \"/home/chardong/y_identification/Venv/BDT_model/skim30/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(datadir+\"RAW_data/Py8_yj_jj_mc16ade_pd122_train_w_skim_30.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "shower_shape_var = ['y_Reta',\n",
    "                    'y_Rphi',\n",
    "                    'y_weta2',\n",
    "                    'y_fracs1',\n",
    "                    'y_weta1',\n",
    "                    'y_wtots1',\n",
    "                    'y_Rhad',\n",
    "                    'y_Rhad1',\n",
    "                    'y_Eratio', \n",
    "                    'y_deltae']\n",
    "\n",
    "conv_var = [ 'y_convRadius', 'y_convType']\n",
    "\n",
    "kinem_var = ['y_pt', 'y_eta', 'y_phi']\n",
    "\n",
    "truth_var = ['y_truth_pt', 'y_truth_eta' ]\n",
    "\n",
    "discriminating_var = shower_shape_var + kinem_var + conv_var \n",
    "\n",
    "Y_var = list(set(df.columns)-set(discriminating_var+truth_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[discriminating_var+truth_var]\n",
    "Y = df[Y_var]\n",
    "\n",
    "x_train_val, x_test, y_train_val, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "weight_train = y_train[\"weight\"]\n",
    "weight_val   = y_val  [\"weight\"]\n",
    "weight_test  = y_test [\"weight\"]\n",
    "\n",
    "Y_var_drop = list(set(Y_var)-{\"truth_label\"})\n",
    "\n",
    "othervars_train = y_train[Y_var_drop]\n",
    "othervars_val   = y_val  [Y_var_drop]\n",
    "othervars_test  = y_test [Y_var_drop]\n",
    "\n",
    "y_train = y_train.drop(Y_var_drop, axis=1)\n",
    "y_test  = y_test .drop(Y_var_drop, axis=1)\n",
    "y_val   = y_val  .drop(Y_var_drop, axis=1)\n",
    "\n",
    "truth_train = x_train[truth_var]\n",
    "truth_val   = x_val  [truth_var]\n",
    "truth_test  = x_test [truth_var]\n",
    "\n",
    "x_train = x_train.drop(truth_var, axis=1)\n",
    "x_test  = x_test .drop(truth_var, axis=1)\n",
    "x_val   = x_val  .drop(truth_var, axis=1)\n",
    "\n",
    "y_train = np.ravel(y_train)\n",
    "y_val   = np.ravel(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py\n",
    "\n",
    "def classification_metric(testy, probs):\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    precision, recall, thresholds = precision_recall_curve(testy, probs[:,1])\n",
    "    # convert to f score\n",
    "    fscore = (2 * precision * recall) / (precision + recall)\n",
    "    # locate the index of the largest f score\n",
    "    ix = np.argmax(fscore)\n",
    "    return fscore[ix]\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    model = lgb.LGBMClassifier(learning_rate  = 0.05,  \n",
    "                               num_leaves     = trial.suggest_int(\"num_leaves\", 2, 256), # tries various value trying to get maximum accuracy\n",
    "                               max_depth      = -5,\n",
    "                               objective      = 'xentropy',\n",
    "                               n_estimators   = 10,\n",
    "                               force_col_wise = True,\n",
    "                               verbosity      = -1)\n",
    "\n",
    "    cb = [lgb.early_stopping(stopping_rounds=10),lgb.log_evaluation(30)]\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "          sample_weight = weight_train,\n",
    "          eval_set = [(x_train, y_train), (x_val, y_val)],  \n",
    "          eval_names = ['Train', 'Validation'],\n",
    "          eval_sample_weight = [weight_train, weight_val],\n",
    "          callbacks = cb)\n",
    "    \n",
    "    y_pred_prob_test = model.predict_proba(x_test)\n",
    "    return classification_metric(y_test, y_pred_prob_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " # Saving model to file\n",
    "joblib.dump(model,'/home/chardong/y_identification/Venv/BDT_model/LGBMClassifier_model_hard_no_loose_lr0.05_35_skim5_optuna.pkl')\n",
    "# Loading model from file\n",
    "model = joblib.load(savedirmodel+\"LGBMClassifier_model_hard_no_loose_lr0.05_35_skim5_optuna.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 16:21:03,525] A new study created in memory with name: no-name-8ed721f0-ceb4-4812-9e28-8b28efe3100d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tTrain's cross_entropy: 0.468612\tValidation's cross_entropy: 0.46487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 16:21:49,165] Trial 0 finished with value: 0.9708853207827065 and parameters: {'num_leaves': 29}. Best is trial 0 with value: 0.9708853207827065.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tTrain's cross_entropy: 0.458198\tValidation's cross_entropy: 0.455849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 16:22:33,894] Trial 1 finished with value: 0.9721366725599038 and parameters: {'num_leaves': 240}. Best is trial 1 with value: 0.9721366725599038.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tTrain's cross_entropy: 0.458267\tValidation's cross_entropy: 0.455896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 16:23:15,619] Trial 2 finished with value: 0.9723457111303252 and parameters: {'num_leaves': 235}. Best is trial 2 with value: 0.9723457111303252.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tTrain's cross_entropy: 0.458382\tValidation's cross_entropy: 0.455961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 16:23:57,033] Trial 3 finished with value: 0.9723002492733159 and parameters: {'num_leaves': 230}. Best is trial 2 with value: 0.9723457111303252.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tTrain's cross_entropy: 0.459632\tValidation's cross_entropy: 0.456885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 16:24:37,533] Trial 4 finished with value: 0.9718374566420876 and parameters: {'num_leaves': 170}. Best is trial 2 with value: 0.9723457111303252.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 5\n",
      "Best trial:\n",
      "  Value: 0.9723457111303252\n",
      "  Params: \n",
      "    num_leaves: 235\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=5)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Feature importance: Numbers of times the feature is used in a model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m lgb\u001b[38;5;241m.\u001b[39mplot_importance(\u001b[43mmodel\u001b[49m, importance_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m'\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature importance: split\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#plt.savefig(savedir+'feature_split_lr_0.09_35_skim30.pdf')\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Feature importance: Numbers of times the feature is used in a model\n",
    "lgb.plot_importance(model, importance_type='split', figsize=(8,6))\n",
    "plt.title('Feature importance: split')\n",
    "#plt.savefig(savedir+'feature_split_lr_0.09_35_skim30.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
